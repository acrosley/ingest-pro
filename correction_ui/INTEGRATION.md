# Integration with Call Processing Pipeline

This document explains how to integrate the Correction UI into your existing call processing workflow.

## Pipeline Overview

```
[Audio File] 
    ↓
[Transcription Engine] (AssemblyAI/Google STT)
    ↓
[Confidence JSON + Transcript TXT]
    ↓
[Review Generator] (assemblyai_review_generator.py)
    ↓
[Flagged Confidence JSON]
    ↓
[Correction UI] ← YOU ARE HERE
    ↓
[Corrected Transcript TXT]
    ↓
[Final Processing / Database]
```

## Step-by-Step Integration

### 1. Prepare Input Files

The Correction UI requires two files:

**A. Confidence JSON** (required)
- Generated by transcription engine
- Must contain `word_data` array with:
  - `word`: The transcribed word
  - `confidence`: Float 0.0-1.0
  - `start_time`: Word start in seconds
  - `end_time`: Word end in seconds
  - `speaker_tag`: Speaker identifier (A, B, etc.)
- Example: `x105_2025-07-29.10-04.027.confidence.json`

**B. Audio File** (optional but recommended)
- Original audio recording
- Formats: WAV, MP3, M4A
- Example: `Alex_2025-07-29_10-04_1m1s_Arliza_Christus Trinity.wav`

### 2. Run Review Generator

Before using the UI, run the review generator to flag important words:

```python
from tools.review_tools.assemblyai_review_generator import (
    generate_assemblyai_review,
    AssemblyAIReviewConfig,
    load_expected_terms
)
from pathlib import Path

# Configure review settings
config = AssemblyAIReviewConfig(
    enabled=True,
    output_directory="output/Review/",
    low_confidence_threshold=0.50,
    flag_phone_numbers=True,
    flag_names=True,
    flag_numbers=True,
    flag_dates=True
)

# Load expected terms (optional)
expected_terms = load_expected_terms(Path("config/nouns_to_expect.txt"))

# Generate review file
confidence_path = Path("output/ALL_JSON_FILES/call_123.confidence.json")
transcript_path = Path("output/ALL_TRANSCRIPT_FILES/call_123.txt")

review_path = generate_assemblyai_review(
    confidence_path,
    transcript_path,
    config,
    expected_terms
)

print(f"Review file created: {review_path}")
```

### 3. Launch Correction UI

**Option A: Manual Launch**
```bash
cd correction_ui
python launch.py
```

**Option B: Automated Launch from Pipeline**
```python
import webbrowser
import subprocess
from pathlib import Path

def launch_correction_ui():
    """Launch the correction UI in browser"""
    ui_dir = Path("correction_ui")
    
    # Start server
    process = subprocess.Popen(
        ["python", "launch.py"],
        cwd=ui_dir,
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE
    )
    
    print("Correction UI launched. Close browser to continue pipeline.")
    return process

# In your pipeline:
if needs_manual_review:
    ui_process = launch_correction_ui()
    ui_process.wait()  # Wait for user to finish corrections
```

### 4. Load Files in UI

Once the UI is open:
1. Click "Load Files" in the sidebar
2. Browse to your confidence JSON file
3. Browse to your audio file
4. Click "Load Files" button

Or use demo data which loads automatically.

### 5. Review and Correct

**For efficient review:**
1. Sort by priority (critical errors first)
2. Use speaker name shortcuts early
3. Focus on high-value flags (phone numbers, dates)
4. Double-click words to verify with audio
5. Use search to find patterns

### 6. Export Corrected Transcript

When finished:
1. Click "Export Corrected Transcript" or press `Ctrl+S`
2. Two files download:
   - `corrected_transcript.txt`: Clean transcript
   - `corrections.json`: Audit log

### 7. Continue Pipeline

Use the corrected transcript for downstream processing:

```python
def process_corrected_transcript(corrected_txt_path, corrections_json_path):
    """Process corrected transcript in pipeline"""
    
    # Load corrected transcript
    with open(corrected_txt_path, 'r', encoding='utf-8') as f:
        corrected_text = f.read()
    
    # Load corrections log
    with open(corrections_json_path, 'r', encoding='utf-8') as f:
        corrections_log = json.load(f)
    
    # Extract metadata
    speaker_names = corrections_log['speakerNames']
    correction_count = corrections_log['stats']['correctionCount']
    
    print(f"Applied {correction_count} corrections")
    print(f"Speakers: {speaker_names}")
    
    # Continue with your pipeline...
    save_to_database(corrected_text, speaker_names)
    generate_summary(corrected_text)
    send_to_client(corrected_text)
```

## Automation Options

### Option 1: Manual Review for All Calls

```python
# In processor.py main pipeline
def process_call(audio_file):
    # Transcribe
    transcript, confidence = transcribe_audio(audio_file)
    
    # Generate review
    review_path = generate_review(confidence)
    
    # STOP HERE - Manual review required
    print(f"Review file ready: {review_path}")
    print("Open Correction UI to review this call")
    input("Press Enter when corrections are complete...")
    
    # Load corrected transcript
    corrected = load_corrected_transcript()
    
    # Continue pipeline
    process_transcript(corrected)
```

### Option 2: Conditional Review (Only Low Confidence)

```python
def process_call(audio_file):
    # Transcribe
    transcript, confidence = transcribe_audio(audio_file)
    
    # Check if review needed
    if needs_review(confidence):
        print("⚠️ Low confidence detected - manual review recommended")
        review_path = generate_review(confidence)
        
        # Launch UI
        launch_correction_ui()
        
        # Wait for corrections
        wait_for_corrections()
        corrected = load_corrected_transcript()
    else:
        print("✅ High confidence - skipping manual review")
        corrected = transcript
    
    # Continue pipeline
    process_transcript(corrected)

def needs_review(confidence_data):
    """Determine if manual review is needed"""
    avg_confidence = confidence_data.get('overall_confidence', 1.0)
    
    # Review if average confidence < 80%
    if avg_confidence < 0.80:
        return True
    
    # Review if any critical flags
    critical_count = count_critical_flags(confidence_data)
    if critical_count > 5:
        return True
    
    return False
```

### Option 3: Batch Review Mode

```python
def batch_review_mode():
    """Process multiple calls, then review all at once"""
    
    # Transcribe all calls
    pending_reviews = []
    for audio_file in get_audio_files():
        transcript, confidence = transcribe_audio(audio_file)
        review_path = generate_review(confidence)
        pending_reviews.append(review_path)
    
    print(f"\n{len(pending_reviews)} calls ready for review")
    print("Open Correction UI and review each call")
    input("Press Enter when all reviews are complete...")
    
    # Process all corrected transcripts
    for review_path in pending_reviews:
        corrected = load_corrected_transcript(review_path)
        finalize_transcript(corrected)
```

## Configuration in assemblyai_review_generator.py

Customize flagging behavior by editing the config:

```python
# In tools/review_tools/assemblyai_review_generator.py

@dataclass
class AssemblyAIReviewConfig:
    # Adjust these thresholds
    low_confidence_threshold: float = 0.50  # Lower = fewer flags
    critical_confidence_threshold: float = 0.30
    
    # Toggle specific flag types
    flag_phone_numbers: bool = True
    flag_case_numbers: bool = True
    flag_money_amounts: bool = True
    flag_dates: bool = True
    flag_times: bool = True
    flag_names: bool = True
    flag_numbers: bool = True
    
    # Context window
    context_words_before: int = 5
    context_words_after: int = 5
```

## Expected Terms Integration

To reduce false positives, maintain a list of expected terms:

**config/nouns_to_expect.txt:**
```
Progressive Insurance
Geico
Allstate
Christus Hospital
Metro Methodist
Dr. Smith
Nurse Johnson
```

These terms won't be flagged even with low confidence.

## Output File Locations

Organize your output files:

```
output/
├── ALL_JSON_FILES/
│   └── call_123.confidence.json        # Input to UI
├── ALL_TRANSCRIPT_FILES/
│   └── call_123.txt                    # Original transcript
├── Review/
│   └── call_123.review.json           # Generated by review_generator
└── Corrected/                         # Create this directory
    ├── call_123_corrected.txt         # Output from UI
    └── call_123_corrections.json      # Audit log from UI
```

## Database Integration

Store corrections in your database:

```python
def save_corrections_to_db(call_id, corrections_json):
    """Save correction metadata to database"""
    
    with open(corrections_json) as f:
        data = json.load(f)
    
    # Update database
    db.execute("""
        UPDATE calls 
        SET 
            corrected_transcript = :transcript,
            correction_count = :count,
            corrected_at = :timestamp,
            corrected_by = :user
        WHERE call_id = :call_id
    """, {
        'call_id': call_id,
        'transcript': load_corrected_transcript(),
        'count': data['stats']['correctionCount'],
        'timestamp': data['timestamp'],
        'user': 'manual_review'
    })
    
    # Log individual corrections for audit
    for word_index, correction in data['corrections'].items():
        db.execute("""
            INSERT INTO transcript_corrections 
            (call_id, word_index, original, corrected, timestamp)
            VALUES (:call_id, :index, :orig, :corr, :time)
        """, {
            'call_id': call_id,
            'index': word_index,
            'orig': get_original_word(word_index),
            'corr': correction,
            'time': data['timestamp']
        })
```

## Quality Metrics

Track correction quality over time:

```python
def calculate_review_metrics(corrections_json):
    """Analyze review session"""
    
    with open(corrections_json) as f:
        data = json.load(f)
    
    metrics = {
        'total_words': data['stats']['totalWords'],
        'corrections_made': data['stats']['correctionCount'],
        'correction_rate': data['stats']['correctionCount'] / data['stats']['totalWords'],
        'speakers_renamed': len(data['speakerNames']),
        'review_duration': calculate_duration(data['timestamp'])
    }
    
    return metrics
```

## Troubleshooting Integration

**Issue: UI can't find files**
- Solution: Use absolute paths or ensure files are in correct relative location
- Check: `../demo/Transcripts/` path works from `correction_ui/`

**Issue: Audio doesn't sync**
- Solution: Ensure audio filename matches confidence JSON
- Check: Audio file is in supported format (WAV, MP3, M4A)

**Issue: Pipeline hangs waiting for review**
- Solution: Add timeout to manual review step
- Implement: Optional "skip review" button in pipeline

**Issue: Corrections not loading in next step**
- Solution: Verify export paths are consistent
- Check: Both TXT and JSON files are saved correctly

## Best Practices

1. **Standardize file naming**: Use consistent naming pattern for easy matching
   ```
   call_123.confidence.json
   call_123.wav
   call_123_corrected.txt
   call_123_corrections.json
   ```

2. **Version control**: Keep original and corrected transcripts separate

3. **Audit trail**: Always save corrections.json for accountability

4. **Training data**: Use corrections to improve transcription models over time

5. **Batch processing**: Review multiple calls in one session for efficiency

6. **Expected terms**: Regularly update nouns_to_expect.txt based on corrections

## Next Steps

After integration:
1. Test with sample calls
2. Measure review time per call
3. Track correction rates
4. Train staff on UI usage
5. Monitor quality improvements

